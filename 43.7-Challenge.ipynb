{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the dataset \n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "#import various components for model building \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "#import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 5us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 6s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "\n",
    "#convert to float32 for type consistency \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "#print sample sizes\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "#convert class vectors to binary class metrices\n",
    "#so instead of one column with 10 values, create 10 binary columns \n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#start with a simple sequetial model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#add dense layers to create a fully connected MLP\n",
    "#note that we specify an input shape for the first layer ONLY\n",
    "#Relu is the activation function used \n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "#dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "#end with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#compile the model to put it all together\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.6512 - accuracy: 0.7701 - val_loss: 0.4636 - val_accuracy: 0.8350\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.4405 - accuracy: 0.8390 - val_loss: 0.4066 - val_accuracy: 0.8537\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3960 - accuracy: 0.8564 - val_loss: 0.4063 - val_accuracy: 0.8561\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.3689 - accuracy: 0.8657 - val_loss: 0.4143 - val_accuracy: 0.8538\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3511 - accuracy: 0.8726 - val_loss: 0.3788 - val_accuracy: 0.8664\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3400 - accuracy: 0.8772 - val_loss: 0.3546 - val_accuracy: 0.8749\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3289 - accuracy: 0.8791 - val_loss: 0.3620 - val_accuracy: 0.8728\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3194 - accuracy: 0.8827 - val_loss: 0.3469 - val_accuracy: 0.8764\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3132 - accuracy: 0.8850 - val_loss: 0.3630 - val_accuracy: 0.8690\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3062 - accuracy: 0.8881 - val_loss: 0.3499 - val_accuracy: 0.8781\n",
      "Test loss: 0.34993864582777023\n",
      "Test accuracy: 0.8780999779701233\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 61s 1ms/step - loss: 0.2577 - accuracy: 0.9212 - val_loss: 0.0499 - val_accuracy: 0.9843\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 59s 978us/step - loss: 0.0863 - accuracy: 0.9749 - val_loss: 0.0395 - val_accuracy: 0.9859\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 56s 938us/step - loss: 0.0639 - accuracy: 0.9814 - val_loss: 0.0355 - val_accuracy: 0.9883\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 57s 942us/step - loss: 0.0541 - accuracy: 0.9836 - val_loss: 0.0318 - val_accuracy: 0.9884\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 57s 949us/step - loss: 0.0459 - accuracy: 0.9860 - val_loss: 0.0289 - val_accuracy: 0.9900\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 58s 964us/step - loss: 0.0422 - accuracy: 0.9871 - val_loss: 0.0288 - val_accuracy: 0.9898\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 58s 958us/step - loss: 0.0360 - accuracy: 0.9892 - val_loss: 0.0292 - val_accuracy: 0.9900\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 57s 948us/step - loss: 0.0335 - accuracy: 0.9893 - val_loss: 0.0277 - val_accuracy: 0.9906\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 57s 958us/step - loss: 0.0317 - accuracy: 0.9905 - val_loss: 0.0269 - val_accuracy: 0.9911\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 57s 947us/step - loss: 0.0309 - accuracy: 0.9904 - val_loss: 0.0298 - val_accuracy: 0.9903\n",
      "Test loss: 0.02984811967540736\n",
      "Test accuracy 0.9902999997138977\n"
     ]
    }
   ],
   "source": [
    "#input image dimensions, from our data\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "#the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "    \n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "#convert class vectors to binary class matrices \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#building the model \n",
    "model = Sequential()\n",
    "#first convolutional layer, note the specification of shape \n",
    "model.add(Conv2D(32, kernel_size=(3,3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 122s 2ms/step - loss: 0.9877 - accuracy: 0.6641 - val_loss: 0.7358 - val_accuracy: 0.7481\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.4307 - accuracy: 0.8637 - val_loss: 0.3000 - val_accuracy: 0.9052\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.2706 - accuracy: 0.9163 - val_loss: 0.3349 - val_accuracy: 0.8906\n",
      "Test loss: 0.3348750020325184\n",
      "Test accuracy: 0.8906000256538391\n"
     ]
    }
   ],
   "source": [
    "#training parameters \n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "\n",
    "#embedding dimensions \n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "#the data, shuffled and split between train and test sets \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#reshape data to 4D for Hierarchical RNN\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "#converts class vectors to binary class matrices \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "#4D input \n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "#encodes a row of pixels using TimeDistributed Wrapper\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "#encodes columns of encoded rows\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "#final predictions and model \n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#training\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test)\n",
    "         )\n",
    "#evaluation \n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN achieves the best accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
